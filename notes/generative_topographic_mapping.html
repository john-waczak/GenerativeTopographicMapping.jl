<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="John Waczak">
<meta name="dcterms.date" content="2023-01-24">

<title>Generative Topographic Mapping</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="generative_topographic_mapping_files/libs/clipboard/clipboard.min.js"></script>
<script src="generative_topographic_mapping_files/libs/quarto-html/quarto.js"></script>
<script src="generative_topographic_mapping_files/libs/quarto-html/popper.min.js"></script>
<script src="generative_topographic_mapping_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="generative_topographic_mapping_files/libs/quarto-html/anchor.min.js"></script>
<link href="generative_topographic_mapping_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="generative_topographic_mapping_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="generative_topographic_mapping_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="generative_topographic_mapping_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="generative_topographic_mapping_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Generative Topographic Mapping</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>John Waczak </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 24, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p><em>Goal</em>: We want models that can find interesting structures in un-structured data without the need for human labeling or intervention. The key feature of many such methods is that they perform <strong>Dimensionality reduction</strong>. Data in some high dimensional feature space are <em>assumed</em> to truly be living on a lower dimensional submanifold of the higher dimensional space. If the goal is to visualize interesting structures in the data, typically, we chose the dimension of the reduced data to be <span class="math inline">2</span>, or <span class="math inline">3</span>.</p>
<p><strong>Generative Topographic Mapping</strong> assumes that our large dimensional dataset <span class="math inline">\mathcal{D}\subseteq \mathbb{R}^D</span> is actually <em>generated</em> by a set of <strong>latent variables</strong> living in <span class="math inline">\mathbb{R}^L</span>. For visualization, we will typically choose <span class="math inline">L=2</span>, but strictly speaking, we require only that <span class="math inline">L&lt;D</span>. Our goal then is to learn the mapping from latent variables to our data space.</p>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation:</h2>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Notation</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\mathbf{x}\in\mathbb{R}^L</span></td>
<td>Latent vector</td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\mathbf{t}_n \in \mathbb{R}^D</span></td>
<td>Data vector</td>
<td><span class="math inline">n=1,..., N</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\mathbf{y}=\mathbf{y}(\mathbf{x};w)</span></td>
<td>latent vector transformation</td>
<td><span class="math inline">\mathbf{y}:\mathbb{R}^L\mapsto\mathbb{R}^D</span></td>
</tr>
<tr class="even">
<td><span class="math inline">w</span></td>
<td>vector/matrix of weights for the mapping transformation</td>
<td></td>
</tr>
</tbody>
</table>
<p>The <strong>GTM</strong> is a principled expansion of both <em>Factor Analysis</em> (the linear latent variable model) and the <em>Self Organizing Map</em> (aka Kohonen Maps).</p>
<p>To expand the capabilities of <em>factor analysis</em> to consider nonlinear transformations of latent vectors, we consider the underlying distribution <span class="math display">\begin{equation}
    p(\mathbf{t}\vert \mathbf{x},w,\beta) := \mathcal{N}(\mathbf{y}(\mathbf{x};w), \frac{1}{\beta})
\end{equation}</span> i.e.&nbsp;the embedded (transformed) latent vectors are normally distributed with mean given by <span class="math inline">\mathbf{y}</span> and covariance <span class="math inline">\beta^{-1}</span>. Provided we have some knowledge of the distribution of latent vectors <span class="math inline">p(\mathbf{x})</span> (aka the Bayesian prior), we can then integrate to obtain</p>
<p><span class="math display">\begin{equation}
    p(\mathbf{t}\vert w, \beta) = \int d\mathbf{x}; p(\mathbf{t}\vert \mathbf{x},w,\beta)p(\mathbf{x})
\end{equation}</span></p>
<p>This is what we really want: the distribution of our embedded data <span class="math inline">\mathbf{t}</span> given the parameters of our model <span class="math inline">w</span> and <span class="math inline">\beta</span>. Unfortunately, this integral is not tractible without a <em>nice</em> model for <span class="math inline">p(\mathbf{x})</span>. We therefore take inspiration from the <em>Self Organizing Map</em> which models latent vectors as the nodes of a topologically connected mesh. Thus, we form</p>
<p><span class="math display">\begin{equation}
    p(\mathbf{x}) := \frac{1}{K}\sum_k^K \delta(\mathbf{x}-\mathbf{x}_k)
\end{equation}</span></p>
<p>In words: the latent dataspace is described by <span class="math inline">K</span> vectors <span class="math inline">\mathbf{x}_k</span> which lie preciesly on the nodes of a regular mesh. For any latent vector <span class="math inline">\mathbf{x}</span>, there is a probability of <span class="math inline">1/K</span> that it <em>came from</em> the <span class="math inline">k^{th}</span> vector <span class="math inline">\mathbf{x}_k</span>.</p>
<p>As mathematicians (or physicists, or data scientists, etc…) we rejoice at the site of the Dirac-<span class="math inline">\delta</span> functions which greatly simplify our integration. Our embedded data distribution now becomes: <span class="math display">\begin{align}
    p(\mathbf{t}\vert w,\beta) &amp;= \int d\mathbf{x}\; p(\mathbf{t}\vert \mathbf{x}, w, \beta) \frac{1}{K}\sum_k^K \delta(\mathbf{x}-\mathbf{x}_k) \\
    &amp;= \frac{1}{K}\sum_k^K p(\mathbf{t}\vert \mathbf{x}_k, w, \beta)
\end{align}</span> which is <em>much</em>, <em>much</em> nicer to deal with.</p>
<p>The question now becomes the following: provided a dataset <span class="math inline">\mathcal{D}=\{\mathbf{t}_1, ..., \mathbf{t}_N \}</span>, what are the best parameters <span class="math inline">w</span> and <span class="math inline">\beta</span> to fit our latent variable model? Assuming each of the <span class="math inline">\mathbf{t}_i</span> are independent and indetically distributed, we can write the <em>likelihood</em> (odds of obtaining parameters <span class="math inline">w</span> and <span class="math inline">\beta</span> given the dataset <span class="math inline">\mathcal{D}</span>) as <span class="math display">\begin{align}
    \mathcal{L}(w,\beta) &amp;:= \prod_n^N p(\mathbf{t}_n\vert w, \beta) \\
    &amp;= \prod_n^N \left[ \frac{1}{K}\sum_k^K p(\mathbf{t}_n\vert \mathbf{x}_k, w, \beta) \right]
\end{align}</span></p>
<p>Sums are easier to manipulate than products and so we make the strategic choice to instead work with the <em>log-likelihood</em> function <span class="math inline">\ell(w,\beta)=\log(\mathcal{L}(w,\beta))</span></p>
<p><span class="math display">\begin{equation}
    \ell(w, \beta) = \sum_n^N \ln\left\{ \frac{1}{K}\sum_k^K p(\mathbf{t}_n\vert \mathbf{x}_k, w, \beta) \right\}
\end{equation}</span></p>
<p>We now have an optimization problem! Our goal is to find those <span class="math inline">w</span> and <span class="math inline">\beta</span> which maximize the likelihood of obtaining our dataset <span class="math inline">\mathcal{D}=\{\mathbf{t}_1, ... , \mathbf{t}_N\}</span> from the latent vectors <span class="math inline">\mathbf{x}_k</span> living on the regular SOM-like grid.</p>
</section>
<section id="summary-so-far" class="level2">
<h2 class="anchored" data-anchor-id="summary-so-far">Summary so far</h2>
<ul>
<li>We treat our data as points, <span class="math inline">\mathbf{t}</span>, living in living in a manifold <span class="math inline">\mathcal{D}</span> which is embedded in some high dimensional space <span class="math inline">\mathbb{R}^D</span>.</li>
<li>We assume our data is Normally distributed in this high dimensional space with mean <span class="math inline">\mathbf{y}(\mathbf{x};w)</span> and covariance <span class="math inline">\beta^{-1}</span></li>
<li>Each of our data points actually <strong>comes from</strong> a lower dimensional latent space of points <span class="math inline">\mathbf{x}</span>.</li>
<li>These lower dimensional points are then mapped by a nonlinear function <span class="math inline">\mathbf{y}=\mathbf{y}(\mathbf{x};w)</span> to the data manifold <span class="math inline">\mathcal{D}</span></li>
<li>We assume that the distribution of latent data is given by a set of discrete points <span class="math inline">\mathbf{x}_k</span> spaced on a regular grid. This was inspired by the Self organizing map and provides the nice topological properties of the GTM.</li>
<li>Given the Bayesian prior for <span class="math inline">p(\mathbf{x})</span>, we then can construct the <em>log-likelihood</em> function <span class="math inline">\ell(w,\beta)</span> which tells us the likelihood of obtaining our data <span class="math inline">\mathbf{t}_n</span> given our model for the latent variable transformation (parametrized by weights <span class="math inline">w</span>) and assumed Gaussian distribution of data points (with covariance <span class="math inline">\beta^{-1}</span>).</li>
<li>Maximizing <span class="math inline">\ell(w, \beta)</span> with respect to <span class="math inline">w</span> and <span class="math inline">\beta</span> will yield our fitted latent-variable model.</li>
<li>We can then use Baye’s rule to invert the distribution and obtain <span class="math inline">p(\mathbf{x_k}\vert \mathbf{t}_n)</span>, i.e.&nbsp;the responsability of the <span class="math inline">k^{th}</span> latent variable for producing the <span class="math inline">n^{th}</span> data point.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We should make the notion of vector, manifold, and point here more precise.</p>
</div>
</div>
</section>
</section>
<section id="em-algorithm-for-gtm" class="level1">
<h1>EM Algorithm for GTM</h1>
<p>Theoretically at this point we have all we need to solve the problem. We can assume some initial guess for <span class="math inline">w</span> and <span class="math inline">\beta</span> and proceed with any of our favorite optimization routines. However the double sum in <span class="math inline">\ell</span> presents some computational complexity that suggests we might seek an alternative optimization scheme. We still have not yet defined the specific form for the nonlinear latent variable transformation <span class="math inline">\mathbf{y}(\mathbf{x}; w)</span> and consequently can utilize the oportunity to manufacture a form for <span class="math inline">\mathbf{y}</span> amenable to an expectation-maximization scheme.</p>
<section id="the-expectation-step" class="level2">
<h2 class="anchored" data-anchor-id="the-expectation-step">The Expectation Step</h2>
<p>To begin, suppose we already have some values for the parameters <span class="math inline">w_{o}</span> and <span class="math inline">\beta_{o}</span> (<span class="math inline">o</span> for <em>old</em>). To make our lives simpler, we’ll write <span class="math inline">\theta :=(w,\beta)</span> to save space. Given these values, we can compute the responsabilities <span class="math inline">r_{kn}</span> <span class="math display">\begin{align}
    r_{kn} := p(\mathbf{x}_k \vert \mathbf{t}_n, \theta_o) &amp;= \frac{p(\mathbf{t}_n\vert \mathbf{x}_k, \theta_o)p(\mathbf{x}_k\vert \theta_0 )}{p(\mathbf{t}_n\vert \theta_0 )} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)p(\mathbf{x}_k \vert \theta_o)}{p(\mathbf{t}_n \vert \theta_o)} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)p(\mathbf{x}_k \vert \theta_o)}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)p(\mathbf{x}_{k'},\theta_o)} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)\frac{p(\mathbf{x}_k,\theta_o)}{p(\theta_o)}}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)p(\mathbf{x}_{k'}\vert\theta_o)} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)\frac{p(\mathbf{x}_k,\theta_o)}{p(\theta_o)}}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)\frac{p(\mathbf{x}_{k'},\theta_o)}{p(\theta_o)}} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)\frac{p(\mathbf{x}_k,\theta_o)}{p(\theta_o)}}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)\frac{p(\mathbf{x}_{k'},\theta_o)}{p(\theta_o)}} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)p(\mathbf{x}_k,\theta_o)}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)p(\mathbf{x}_{k'},\theta_o)} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)p(\mathbf{x}_k)p(\theta_o)}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)p(\mathbf{x}_{k'})p(\theta_o)} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)p(\mathbf{x}_k)}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)p(\mathbf{x}_{k'})} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)\frac{1}{K}}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)\frac{1}{K}} \\
    &amp;= \frac{p(\mathbf{t}_n \vert \mathbf{x}_k, \theta_o)}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},\theta_o)} \\
\end{align}</span> <span class="math display">\begin{equation}
    \boxed{r_{kn} = \frac{p(\mathbf{t}_n\vert \mathbf{x}_k, \theta_o)}{\sum_{k'}^K p(\mathbf{t}_n \vert \mathbf{x}_{k'}, \theta_o)}},
\end{equation}</span></p>
<p>or in words, <span class="math inline">r_{kn}</span> is the posterior i.e.&nbsp;the probability that the <span class="math inline">n^{th}</span> data point <em>came from</em> the <span class="math inline">k^{th}</span> latent node <span class="math inline">\mathbf{x}_k</span>. This is the expectation step</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The latent variable points <span class="math inline">\mathbf{x}_k</span> don’t move but their transformed locations in the embedded data manifold <em>do</em> as the weights <span class="math inline">w</span> are updated.</p>
</div>
</div>
</section>
<section id="deciding-on-a-form-for-mathbfy" class="level2">
<h2 class="anchored" data-anchor-id="deciding-on-a-form-for-mathbfy">Deciding on a form for <span class="math inline">\mathbf{y}</span></h2>
<p><span class="math inline">\mathbf{y}</span> can be any nonlinear, parametric model (for example a multi-layer-perceptron). FOr convenience, we choose a generalized (kernalized) linear regression: <span class="math display">\begin{equation}
    \mathbf{y} := W\phi(\mathbf{x})
\end{equation}</span> where <span class="math inline">W\in\mathbb{R}^{D\times M}</span> and <span class="math inline">\phi(\mathbf{x})\in\mathbb{R}^M</span>. Here <span class="math inline">\phi_m</span> is the <span class="math inline">m^{th}</span> basis function applied to <span class="math inline">\mathbf{x}</span>. In order to capture linear <em>and</em> nonlinear effects, a combination of linear basis functions and Radial Basis Functions (RBF) are used so that <span class="math display">\begin{equation}
    \phi_m(\mathbf{x}) = \begin{cases}
        \exp(- \frac{1}{2\sigma}\lvert \mathbf{x}-\mathbf{\mu}_m\rvert^2) &amp; m \leq M_{NL} \\
        \mathbf{x}^{(l)} &amp; m = M_{NL} + l, \qquad l\in 1, ..., L \\
        1 &amp; m = M_{NL} + L + 1 = M
    \end{cases}
\end{equation}</span> i.e.&nbsp;a combination of <span class="math inline">M_{NL}</span> gausians with centers <span class="math inline">\mathbf{\mu}_m</span> and <span class="math inline">L</span> linear functions. For convenience, we can write this in matrix form: <span class="math display">\begin{equation}
    Y = \Phi W
\end{equation}</span> with <span class="math inline">\Phi \in \mathbb{R}^{K\times M}</span> so that <span class="math inline">\Phi_{km} = \phi_m(\mathbf{x}_k)</span>.</p>
</section>
<section id="the-maximization-step" class="level2">
<h2 class="anchored" data-anchor-id="the-maximization-step">The Maximization Step</h2>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We should add an derivation that demonstrates why EM is <em>guarenteed</em> to increase the log-likelihood function.</p>
</div>
</div>
<p>We now derive the maximization step, or in other words, given the previous values <span class="math inline">\theta_o</span> and the responsabilities <span class="math inline">r_{kn}</span>, how do we obtain <span class="math inline">\theta_n</span> (n for new) so that our log-likelihood function increases? We differentiate <span class="math inline">\ell</span> with respect to the parameters while treating <span class="math inline">r_{kn}</span> as constant: <span class="math display">\begin{align}
    \ell(w,\beta) &amp;= \sum_n^N \ln \left(\frac{1}{K}\sum_k^K p(\mathbf{t}_n\vert \mathbf{x}_k, w, \beta ) \right) \\
    0 &amp;= \frac{\partial}{\partial w_{md}}\ell(w, \beta) \\
    &amp;= \frac{\partial}{\partial w_{md}} \sum_n^N \ln \left(\frac{1}{K}\sum_k^K p(\mathbf{t}_n\vert \mathbf{x}_k, w, \beta ) \right) \\
    &amp;= \sum_n^N \frac{1}{\frac{1}{K}\sum_{k'}^Kp(\mathbf{t}_n\vert \mathbf{x}_{k'}, w, \beta)}\frac{1}{K}\frac{\partial}{\partial w_{md}}\sum_k^K p(\mathbf{t}_n \vert \mathbf{x}_k, w, \beta) \\
    &amp;= \sum_n^N\sum_k^K \frac{ \frac{\partial}{\partial w_{md}} p(\mathbf{t}_n\vert\mathbf{x}_k,w,\beta)  }{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},w,\beta)} \\
    &amp;= \sum_n^N\sum_k^K \frac{p(\mathbf{t}_n\vert \mathbf{x}_k, w, \beta)}{\sum_{k'}^K p(\mathbf{t}_n\vert \mathbf{x}_{k'},w,\beta)} \frac{\partial}{\partial w_{md}} \left\{ \frac{-\beta}{2}\sum_d^D \left(t_n^{(d)} - y_k^{(d)} \right)^2 \right\}  \\
    &amp;= \sum_n^N\sum_k^K r_{nk} \frac{\partial}{\partial w_{md}} \left\{ \frac{-\beta}{2}\sum_d^D \left(t_n^{(d)} - y_k^{(d)} \right)^2 \right\}  \\
    &amp;= \sum_n^N\sum_k^K r_{nk}(-\beta)  \sum_d^D \left(t_n^{(d)} - y_k^{(d)} \right) \frac{\partial y_k^{(d)}}{\partial w_{md}} \\
    &amp;= \sum_n^N\sum_k^K r_{nk}\beta   \sum_d^D \left( y_k^{(d)} - t_n^{(d)} \right) \frac{\partial}{\partial w_{md}}\sum_{s}^M\phi_s(\mathbf{x}_k)w_{sd} \\
    &amp;= \sum_n^N\sum_k^K r_{nk}\beta   \sum_d^D \left( y_k^{(d)} - t_n^{(d)} \right) \sum_{s}^M\phi_s(\mathbf{x}_k)\delta_{ms} \\
    &amp;= \sum_n^N\sum_k^K r_{nk}\beta   \sum_d^D \left( y_k^{(d)} - t_n^{(d)} \right) \phi_m(\mathbf{x}_k) \\
    &amp;= \sum_n^N \sum_k^K \sum_d^D \beta \; r_{nk}  \left( y_k^{(d)} - t_n^{(d)} \right) \phi_m(\mathbf{x}_k) \\
\end{align}</span></p>
<p>Let us now define the diagonal matrix <span class="math inline">G</span> given by <span class="math inline">G_{kk} = \sum_n r_{nk}</span>. Then the system of equations has become: <span class="math display">\begin{align}
    \sum_n \sum_k \sum_d r_{nk} y_k^{(d)}\phi_m(\mathbf{x}_k) &amp;= \sum_n \sum_k \sum_d r_{nk}t_n^{(d)}\phi_m(\mathbf{x}_k) \\
    \sum_n\sum_k\sum_d r_{nk} \left( \sum_s \phi_s(\mathbf{x}_k)w_{sd} \right) \phi_m(\mathbf{x}_k) &amp;= \sum_n \sum_k \sum_d r_{nk}t_n^{(d)}\phi_m(\mathbf{x}_k) \\
    \sum_k\sum_d\sum_s G_{kk}\Phi_{sk}w_{sd}\Phi_{mk} &amp;= \sum_n\sum_k\sum_d r_{nk}t_{nd}\Phi_{mk}
\end{align}</span> where <span class="math inline">\Phi</span> is the matrix with components <span class="math inline">\Phi_{mk} = \phi_m(\mathbf{x}_k)</span>. Similarly define <span class="math inline">R</span> as the matrix with components <span class="math inline">R_{nk}=r_{nk}</span> and <span class="math inline">T</span> as the data matrix with components <span class="math inline">T_{nd}=t_n^{(d)}</span>. With these convenctions, the entire system can be written as <span class="math display">\begin{equation}
    \Phi^T G \Phi W = \Phi^T R T
\end{equation}</span></p>
<p>Performing the same procedure for the derivative w.r.t. <span class="math inline">\beta</span>, we find <span class="math display">\begin{equation}
    \frac{1}{\beta} = \frac{1}{ND}\sum_n^N\sum_k^K r_{kn} \lVert \mathbf{y}(\mathbf{x}_k, w) - \mathbf{t}_n\rVert^2
\end{equation}</span></p>
<p>Thus, once we have determined the responsabilities via the <em>expectation step</em>, we perform the maximization step via <span class="math display">\begin{equation}
    \boxed{ \Phi^T G_{\text{old}}\Phi W_{\text{new}} = \Phi^TR_{\text{old}}T}
\end{equation}</span> <span class="math display">\begin{equation}
    \boxed{\frac{1}{\beta_{\text{new}}} = \frac{1}{ND}\sum_n^N\sum_k^K r_{kn}^{\text{(old)}}\lVert \phi(\mathbf{y}(\mathbf{x}_k, W_{\text{new}}) - \mathbf{t}_n \rVert^2}
\end{equation}</span></p>
</section>
<section id="initialization-for-the-em-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="initialization-for-the-em-algorithm">Initialization for the EM Algorithm</h2>
<p>The last needed piece to be able to fit our GTM model is a plan for how to initialize the model parameters in <span class="math inline">W_{md}</span> and <span class="math inline">\beta</span>. A simple strategy is to randomnly initialize the weights before training. Once can do a slightly better job by forcing the weight variance to yield a variance in the projected values <span class="math inline">\mathbf{y}</span> that matches the variance of the data. This is easily accomplished by means of principal component analysis. We use the following strategy:</p>
<ol type="1">
<li>Obtain the data covariance matrix <code>U</code> whose columns are the principal components of our dataset <span class="math inline">\mathcal{D}</span>.</li>
<li>Keep only the first two principal components as our latent space will be 2-dimensional.</li>
<li>Fit <span class="math inline">W</span> so that <span class="math inline">W\Phi \approx UX^T</span>.</li>
<li>Set <span class="math inline">\beta^{-1}</span> to be the variance of the third principal component vector.</li>
</ol>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We should check the dimensions of the equation in <code>3.</code> so that everything matches.</p>
</div>
</div>
</section>
<section id="em-algorithm-summary" class="level2">
<h2 class="anchored" data-anchor-id="em-algorithm-summary">EM Algorithm Summary</h2>
<ol type="1">
<li>Generate a grid of latent points <span class="math inline">\{ \mathbf{x}_k \}</span> with <span class="math inline">k=1,...,K</span>.</li>
<li>Generate a grid of basis function centers <span class="math inline">\{\mathbf{\mu}_m\}</span> with <span class="math inline">m=1, ..., M</span>.</li>
<li>Select the basis function width <span class="math inline">\sigma</span>.</li>
<li>Compute the matrix of basis function activations <span class="math inline">\Phi</span> where <span class="math inline">\Phi_{mk} = \Phi_m(\mathbf{x}_k)</span>.</li>
<li>Initialize the weight matrix <span class="math inline">W</span> randomly or with PCA</li>
<li>Initialize <span class="math inline">\beta</span>.</li>
<li>If desired, select a value <span class="math inline">\alpha</span> to allow for weight regularization, i.e.&nbsp;<span class="math inline">(\Phi^TG\Phi + \frac{\alpha}{\beta}I)W = \Phi^TRT</span>. This corresponds to specifying a Bayesian prior distribution over <span class="math inline">W</span> with <span class="math display">\begin{equation}
p(W) = \left(\frac{\alpha}{2\pi}\right)^{W/2}\exp(-\frac{\alpha}{2}\lVert W \rVert^2)
\end{equation}</span></li>
<li>Compute difference matrix <span class="math inline">\mathbf{\Delta}</span> with <span class="math inline">\Delta_{kn} := \lVert \mathbf{t}_n - \phi_k W \rVert^2</span></li>
<li>Repeat the following until <em>convergence</em>
<ol type="1">
<li>Compute the responsability matrix <span class="math inline">R</span> using <span class="math inline">\Delta</span> and <span class="math inline">\beta</span></li>
<li>Compute <span class="math inline">G</span> from the responsability matrix <span class="math inline">R</span></li>
<li>Update the weight matrix <span class="math inline">W</span> with <span class="math inline">W = (\Phi^TG\Phi)^{-1}\Phi^TRT</span></li>
<li>Compute updated <span class="math inline">\mathbf{\Delta}</span></li>
<li>Update <span class="math inline">\beta</span></li>
</ol></li>
</ol>
</section>
</section>
<section id="visualization-of-results" class="level1">
<h1>Visualization of Results</h1>
<p>Once we have fit a GTM model to our dataset <span class="math inline">\mathcal{D}</span> and determined suitable parameters <span class="math inline">W</span> and <span class="math inline">\beta</span>, we can define a distribution in the data space conditioned on our latent variables <span class="math inline">\mathbf{x}_k</span>, that is <span class="math inline">p(\mathbf{t}\vert \mathbf{x}_k</span> for <span class="math inline">k=1,...,K</span>. Using Baye’s theorem, we can then compute the posterior distribution in the <em>latent space</em> for any point in the data space, in other words, given a datapoint <span class="math inline">\mathbf{t}</span>, how much is it <em>explained</em> by the <span class="math inline">k^{th}</span> latent variable <span class="math inline">p(\mathbf{x}_k \vert \mathbf{t}</span>. For each datapoint, this would result in a matrix of responsabilities <span class="math inline">R</span>. This is probably to cumbersome to visualize for each point, so instead one may resort to visualizing the mean and mode of the distribution: <span class="math display">\begin{align}
    \langle \mathbf{x} \vert \mathbf{t}_n, W, \beta \rangle &amp;= \int p(\mathbf{x}\vert \mathbf{t}_n, W, \beta) \mathbf{x}d\mathbf{x} \\
    &amp;= \int \frac{p(\mathbf{t}_n\vert \mathbf{x}, W, \beta)p(\mathbf{x})}{\sum_{k'}p(\mathbf{t}_n\vert \mathbf{x}_{k'}, W, \beta)p(\mathbf{x}_{k'})}\mathbf{x}d\mathbf{x}\\
    &amp;= \int \frac{p(\mathbf{t}_n\vert \mathbf{x}, W, \beta)p(\mathbf{x})}{\sum_{k'}p(\mathbf{t}_n\vert \mathbf{x}_{k'}, W, \beta)p(\mathbf{x}_{k'})}\mathbf{x}d\mathbf{x}\\
    &amp;= \int \frac{p(\mathbf{t}_n\vert \mathbf{x}, W, \beta)\frac{1}{K}\sum_k\delta(\mathbf{x}-\mathbf{x}_k)}{\sum_{k'}p(\mathbf{t}_n\vert \mathbf{x}_{k'}, W, \beta)p(\mathbf{x}_{k'})}\mathbf{x}d\mathbf{x}\\
    &amp;= \sum_k R_{kn} \mathbf{x}_k
\end{align}</span> <span class="math display">\begin{equation}
    \boxed{\langle \mathbf{x} \vert \mathbf{t}_n, W, \beta \rangle = \sum_k^K R_{kn}\mathbf{x}_k }
\end{equation}</span></p>
<p>If the distribution is multimodal, it is also advantageous to compute the mode: <span class="math display">\begin{align}
    \boxed{\mathbf{x}_{\text{mode}} = \mathbf{x}_{k^{\text{max}}} \;\;\text{where}\;\; k^{\text{max}} = \underset{\{k\}}{\text{argmax}} R_{kn}}
\end{align}</span></p>
</section>
<section id="coding-things-up" class="level1">
<h1>Coding Things Up!</h1>
<div class="cell" data-execution_count="201">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Pkg </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">Pkg</span>.<span class="fu">activate</span>(<span class="st">"."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>  Activating project at `~/gitrepos/machine-learning/GTM.jl/notes`</code></pre>
</div>
</div>
<div class="cell" data-execution_count="202">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Plots</span>, <span class="bu">LinearAlgebra</span>, <span class="bu">Statistics</span>, <span class="bu">Distributions</span>, <span class="bu">MLJ</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Create a sample dataset using MLJ. We will use a classification set so that we can see if GTM can distinguish the classes after fitting.</p>
<p>The table <code>X</code> contains our data (each row is one datum) with class labels in <code>y</code></p>
<div class="cell" data-execution_count="203">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Tables</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>DataX,Datay<span class="op">=</span> <span class="fu">make_blobs</span>(<span class="fl">500</span>, <span class="fl">10</span>; centers<span class="op">=</span><span class="fl">5</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>𝒟 <span class="op">=</span> Tables.<span class="fu">matrix</span>(DataX)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="203">
<pre><code>500×10 Matrix{Float64}:
  5.8199   -0.23623      5.95858   …   6.59544  -0.736488   2.55719
 -3.75379   4.73507    -10.0517        6.59447  -1.83786   -2.91499
 -5.25946  -0.0990778    0.211652     -9.92081  -4.07789    2.46497
 -6.16105   8.47455      9.40879      -4.53639   5.3206     5.95747
  6.16938  -1.21366      4.85269       7.51401  -1.62565    4.25148
 -5.9724    9.1843       8.11657   …  -6.27974   7.19353    7.31728
  5.11808  -1.42707      5.14949       7.82429  -1.52409    3.36089
 -1.78893  -5.15365     -0.662932     -3.18957   9.00017   -3.81975
 -1.00818  -2.86494      0.343802     -3.89628  10.4125    -6.1042
  4.66625   0.058438     5.74507       7.57577  -4.32329    4.48464
  4.16042  -0.606794     5.42266   …   7.49471  -2.35107    5.15123
 -2.80741   4.54917     -6.11905       5.47836  -1.92685   -1.74805
 -2.56221  -3.71248     -0.777029     -4.48369   9.50519   -7.16659
  ⋮                                ⋱                       
 -3.95958   3.18199     -7.24262       5.23833  -2.08023   -3.74225
 -7.80112   8.90108      8.44736      -3.72501   6.97767    6.76299
 -8.0141    9.61691     11.1378    …  -4.29061   4.79243    6.03762
 -3.06065  -5.48419     -1.36768      -3.62517   8.05097   -4.4286
 -3.01497   4.70408     -9.61578       6.1013   -4.2373    -3.86683
  6.21821  -2.7859       7.06395       7.04389  -2.97884    7.03808
  5.57213  -0.915033     3.2505        6.96442  -3.7197     7.09872
 -7.3109    1.01612      3.21817   …  -7.27703  -6.09201    2.46387
 -6.81073  11.2765       9.76835      -4.07706   5.85822    7.20896
 -6.27491   0.398253     0.485098     -7.09156  -3.92557    1.5792
 -1.46093  -4.77424     -0.958444     -4.2627   10.0929    -5.18338
 -1.63404  -6.41701     -0.105115     -2.16537   8.01982   -5.70245</code></pre>
</div>
</div>
<div class="cell" data-execution_count="204">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. GTM  parameters</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="fl">10</span>  <span class="co"># there are K=k² total latent nodes</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="fl">5</span>   <span class="co"># there are M=m² basis functions (centers of our RBFs) </span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>σ <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># the σ² is the variance in our RBFs</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>n_nodes <span class="op">=</span> k<span class="op">*</span>k </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>n_rbf_centers <span class="op">=</span> m<span class="op">*</span>m </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="204">
<pre><code>25</code></pre>
</div>
</div>
<div class="cell" data-execution_count="205">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Create node matrix X</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">range</span>(<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>, length<span class="op">=</span>k) </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> <span class="fu">vcat</span>([x[i] for i <span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x), j <span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x)]<span class="op">...</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> <span class="fu">vcat</span>([x[j] for i <span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x), j<span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x)]<span class="op">...</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="fu">hcat</span>(xs, ys)  <span class="co"># X[:,1] are the x positions, X[:,2] are the y positions</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="fu">size</span>(X,<span class="fl">1</span>) <span class="op">==</span> n_nodes</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fu">scatter</span>(X[<span class="op">:</span>,<span class="fl">1</span>], X[<span class="op">:</span>,<span class="fl">2</span>], color<span class="op">=:</span>black, label<span class="op">=</span><span class="st">"node locations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="205">
<p><img src="generative_topographic_mapping_files/figure-html/cell-6-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="206">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create rbf centers matrix M</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">range</span>(<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>, length<span class="op">=</span>m) </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> <span class="fu">vcat</span>([x[i] for i <span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x), j <span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x)]<span class="op">...</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> <span class="fu">vcat</span>([x[j] for i <span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x), j<span class="op">∈</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span>(x)]<span class="op">...</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="fu">hcat</span>(xs, ys)  <span class="co"># X[:,1] are the x positions, X[:,2] are the y positions</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">size</span>(M,<span class="fl">1</span>) <span class="op">==</span> n_rbf_centers</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="fu">scatter!</span>(M[<span class="op">:</span>,<span class="fl">1</span>], M[<span class="op">:</span>,<span class="fl">2</span>], color<span class="op">=:</span>purple, label<span class="op">=</span><span class="st">"rbf centers"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="206">
<p><img src="generative_topographic_mapping_files/figure-html/cell-7-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="207">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Initialize rbf width</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the rbf variance to the mean squared distance between rbf centers</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distances</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>σ <span class="op">=</span> <span class="fu">mean</span>(<span class="fu">sqeuclidean</span>(M[<span class="op">:</span>,<span class="fl">1</span>], M[<span class="op">:</span>,<span class="fl">2</span>]))  <span class="co"># sqeuclidean is Squared Euclidean distance</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="207">
<pre><code>25.0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="208">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Create rbf matrix Φ</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># this matrix contains the RBF basis functions applied to each latent point *plus* one constant row to enable fitting a bias. </span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>Φ <span class="op">=</span> <span class="fu">zeros</span>(n_nodes, n_rbf_centers <span class="op">+</span> <span class="fl">1</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="op">∈</span> <span class="fl">1</span><span class="op">:</span>n_rbf_centers, i<span class="op">∈</span><span class="fl">1</span><span class="op">:</span>n_nodes</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    Φ[i,j] <span class="op">=</span> <span class="fu">exp</span>(<span class="fu">-sqeuclidean</span>(X[i,<span class="op">:</span>], M[j,<span class="op">:</span>])<span class="op">/</span>(<span class="fl">2</span>σ))</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># set the last column to ones </span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>Φ[<span class="op">:</span>,<span class="kw">end</span>] <span class="op">.=</span> <span class="fl">1</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>Φ</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="208">
<pre><code>100×26 Matrix{Float64}:
 1.0       0.995012  0.980199  0.955997  …  0.904837  0.882497  0.852144  1.0
 0.999013  0.998458  0.987974  0.967873     0.912015  0.893459  0.866572  1.0
 0.996057  0.999938  0.993846  0.977963     0.917436  0.902773  0.879506  1.0
 0.991151  0.999445  0.99778   0.986207     0.921067  0.910384  0.890871  1.0
 0.984322  0.99698   0.999753  0.992559     0.922888  0.916247  0.900602  1.0
 0.975611  0.992559  0.999753  0.99698   …  0.922888  0.920328  0.908643  1.0
 0.965069  0.986207  0.99778   0.999445     0.921067  0.922604  0.914947  1.0
 0.952757  0.977963  0.993846  0.999938     0.917436  0.923059  0.919477  1.0
 0.938746  0.967873  0.987974  0.998458     0.912015  0.921693  0.922205  1.0
 0.923116  0.955997  0.980199  0.995012     0.904837  0.918512  0.923116  1.0
 0.999013  0.99403   0.979231  0.955054  …  0.920158  0.897439  0.866572  1.0
 0.998027  0.997472  0.986999  0.966918     0.927457  0.908587  0.881245  1.0
 0.995074  0.998951  0.992865  0.976997     0.93297   0.918059  0.894398  1.0
 ⋮                                       ⋱                                ⋮
 0.881245  0.908587  0.927457  0.937299     0.986999  0.997472  0.998027  1.0
 0.866572  0.897439  0.920158  0.934064     0.979231  0.99403   0.999013  1.0
 0.923116  0.918512  0.904837  0.882497  …  0.980199  0.955997  0.923116  1.0
 0.922205  0.921693  0.912015  0.893459     0.987974  0.967873  0.938746  1.0
 0.919477  0.923059  0.917436  0.902773     0.993846  0.977963  0.952757  1.0
 0.914947  0.922604  0.921067  0.910384     0.99778   0.986207  0.965069  1.0
 0.908643  0.920328  0.922888  0.916247     0.999753  0.992559  0.975611  1.0
 0.900602  0.916247  0.922888  0.920328  …  0.999753  0.99698   0.984322  1.0
 0.890871  0.910384  0.921067  0.922604     0.99778   0.999445  0.991151  1.0
 0.879506  0.902773  0.917436  0.923059     0.993846  0.999938  0.996057  1.0
 0.866572  0.893459  0.912015  0.921693     0.987974  0.998458  0.999013  1.0
 0.852144  0.882497  0.904837  0.918512     0.980199  0.995012  1.0       1.0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="209">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. perform PCA on data to set up linear basis functions. </span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">MultivariateStats</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset has rows as our records. PCA wants them as the columns</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="fu">size</span>(𝒟)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> <span class="fu">fit</span>(PCA, 𝒟<span class="ch">'; maxoutdim=3)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="209">
<pre><code>PCA(indim = 10, outdim = 3, principalratio = 0.8484339613384994)

Pattern matrix (unstandardized loadings):
────────────────────────────────────
          PC1         PC2        PC3
────────────────────────────────────
1   -2.20938    3.09024    -1.06725
2    2.85454    0.0343929   3.82253
3    4.77197    3.32616    -1.42665
4   -1.58227   -2.00701     3.29562
5    0.93737    1.8949      0.485669
6   -0.403139   2.87226     0.212774
7   -2.81815    1.4322     -2.90989
8   -3.9994     3.46003     1.99449
9    2.2626    -2.07305    -2.75982
10   3.05605    3.1504      1.44111
────────────────────────────────────

Importance of components:
──────────────────────────────────────────────────────────
                                 PC1        PC2        PC3
──────────────────────────────────────────────────────────
SS Loadings (Eigenvalues)  77.7422    64.7283    51.0672
Variance explained          0.340808   0.283757   0.223869
Cumulative variance         0.340808   0.624565   0.848434
Proportion explained        0.40169    0.334448   0.263862
Cumulative proportion       0.40169    0.736138   1.0
──────────────────────────────────────────────────────────</code></pre>
</div>
</div>
<div class="cell" data-execution_count="210">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. Set U matrix to first two principle axes (since latent space is two dimensional)</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>pca_vecs <span class="op">=</span> <span class="fu">projection</span>(pca)  <span class="co"># this results the princiap component vectors (columns) sorted in order of explained variance </span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>pca_var <span class="op">=</span> <span class="fu">principalvars</span>(pca)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">size</span>(pca_vecs)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="fu">size</span>(pca_var)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> pca_vecs[<span class="op">:</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">2</span>] </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="op">∈</span> <span class="fu">axes</span>(U,<span class="fl">2</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    U[<span class="op">:</span>,i] <span class="op">.=</span> <span class="fu">sqrt</span>(pca_var[i])<span class="op">.*</span>U[<span class="op">:</span>,i]  <span class="co"># we still need to figure out why they're scaling by the explained variance...</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="210">
<pre><code>51.067161215523726</code></pre>
</div>
</div>
<div class="cell" data-execution_count="211">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>U</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="211">
<pre><code>10×2 Matrix{Float64}:
 -2.20938    3.09024
  2.85454    0.0343929
  4.77197    3.32616
 -1.58227   -2.00701
  0.93737    1.8949
 -0.403139   2.87226
 -2.81815    1.4322
 -3.9994     3.46003
  2.2626    -2.07305
  3.05605    3.1504</code></pre>
</div>
</div>
<div class="cell" data-execution_count="293">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 8. Initialize parameter matrix W using Φ, and U</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">Init_W_Matrix</span>(X,Φ,U)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We want to find W such that WΦ' = UX'</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># therefore, W' is the solution to Φ'⋅Φ⋅W' = Φ'UX'</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((Φ<span class="ch">'*Φ)\(Φ'</span><span class="op">*</span>X<span class="op">*</span>U<span class="op">'</span>))<span class="ch">'</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="293">
<pre><code>Init_W_Matrix (generic function with 1 method)</code></pre>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I have done this differently thant than <code>ugtm.py</code>. They standardize the vectors first. I think they also have a typo in how they solve for W (they leave <code>UX'</code> out until the end. See <a href="https://github.com/hagax8/ugtm/blob/master/ugtm/ugtm_core.py#L145">this link</a>. Maybe this is just an approximation to speed things up.</p>
</div>
</div>
<div class="cell" data-execution_count="294">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> <span class="fu">Init_W_Matrix</span>(X,Φ,U)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="294">
<pre><code>10×26 adjoint(::Matrix{Float64}) with eltype Float64:
    -66.1707    -8869.24         2221.28  …     715.413    4175.36   -4.76914
   9797.21     -52907.3         13479.9      -21922.4     36418.9   -24.4541
  24260.6     -140689.0         35804.9      -53691.1     94819.9   -65.7297
 -10224.7       61100.5        -15542.9       22517.1    -40827.9    28.6682
   7759.74     -47480.4         12074.0      -17020.5     31517.0   -22.3505
   5554.78     -38514.0          9777.24  …  -11905.8     24729.9   -18.4196
  -6136.49      28798.5         -7355.45      13998.0    -20731.3    12.9958
  -5266.11      18053.3         -4642.94      12422.1    -14599.1     7.59055
   2700.4       -8365.46         2157.29      -6424.78     7058.78   -3.41529
  17997.5          -1.06408e5   27072.5      -39704.9     71318.4   -49.8515</code></pre>
</div>
</div>
<div class="cell" data-execution_count="295">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> U<span class="op">*</span>X<span class="op">'</span><span class="fu">*</span>((Φ<span class="ch">'*Φ)\ Φ'</span>)<span class="ch">'  # this is how they do it... </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="295">
<pre><code>10×26 Matrix{Float64}:
    -68.1991   -8854.08         2215.59  …     720.702    4165.11   -4.76126
   9797.54    -52909.4         13481.4      -21923.0     36420.0   -24.4549
  24259.6         -1.4068e5    35803.1      -53687.7     94813.2   -65.7244
 -10223.9      61093.7        -15540.9       22514.6    -40822.9    28.6643
   7758.81    -47473.1         12071.7      -17017.8     31511.8   -22.3465
   5553.24    -38502.0          9773.03  …  -11901.6     24721.6   -18.4132
  -6137.65     28806.9         -7359.05      14000.9    -20736.8    13.0
  -5268.47     18070.9         -4649.99      12428.2    -14610.8     7.59956
   2701.87     -8376.29         2161.54      -6428.52     7066.03   -3.42083
  17996.2         -1.06397e5   27069.7      -39700.9     71310.5   -49.8453</code></pre>
</div>
</div>
<div class="cell" data-execution_count="297">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 9. Initialize Y using W and Φ</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> W<span class="op">*</span>Φ<span class="ch">'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="297">
<pre><code>10×100 Matrix{Float64}:
 -0.880747  -1.37201   -1.86323   …   1.86237    1.37159    0.880852
 -2.88449   -2.25557   -1.62405       1.61677    2.25385    2.89299
 -8.08698   -7.04031   -5.98714       5.96816    7.03559    8.10819
  3.58456    3.2388     2.8903       -2.88213   -3.23673   -3.59353
 -2.82867   -2.62485   -2.41895       2.41263    2.62323    2.83549
 -2.46647   -2.55942   -2.65086   …   2.64589    2.55805    2.47143
  1.38324    0.760232   0.135613     -0.131485  -0.759356  -1.38849
  0.537158  -0.349048  -1.2366        1.23948    0.349488  -0.541541
 -0.188432   0.313092   0.815299     -0.816688  -0.313276   0.190664
 -6.19814   -5.5293    -4.85563       4.84135    5.52571    6.21391</code></pre>
</div>
</div>
<div class="cell" data-execution_count="303">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 10. Set noise varaiance parameter</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>β⁻¹ <span class="op">=</span> <span class="fu">maximum</span>([<span class="fu">mean</span>(<span class="fu">pairwise</span>(sqeuclidean, Y, dims<span class="op">=</span><span class="fl">2</span>))<span class="op">/</span><span class="fl">2</span>, pca_var[<span class="kw">end</span>]])</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  β⁻¹ = pca_var[end]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="303">
<pre><code>58.04360187458589</code></pre>
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>