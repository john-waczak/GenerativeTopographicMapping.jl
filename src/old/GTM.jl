using LinearAlgebra
using Statistics
using Distances
using MultivariateStats



# ----------------------------------------------
#     Initialization functions
# ----------------------------------------------



"""
    getCoordsMatrix(k::Int)

Generate a matrix of `kÂ²` node coordinates on a regular grid with ``xâˆˆ [-1,1] `` and ``yâˆˆ[-1, 1]``. 
"""
function getCoordsMatrix(k::Int)
    # return size: (k^2, 2)

    x = range(-1.0, 1.0, length=k)
    xs = vcat([x[i] for i âˆˆ 1:length(x), j âˆˆ 1:length(x)]...)
    ys = vcat([x[j] for i âˆˆ 1:length(x), jâˆˆ 1:length(x)]...)
    X = hcat(xs, ys)  # X[:,1] are the x positions, X[:,2] are the y positions
end



"""
    initializeVariance(Ïƒ::Float64, M::)

Initilize RBF variance by combining supplied standard deviation with minimum RBF mean distance.
"""
function initializeVariance(Ïƒ, M)
    mdist = pairwise(euclidean, M, M, dims=1)
    minimum(mdist[mdist .> 0.0])
    ÏƒÂ² = Ïƒ * minimum(mdist[mdist .> 0.0])
    return ÏƒÂ²
end



"""
    getÎ¦Matrix(X, M, ÏƒÂ²)

Given a matrix of latent node coordinates `X`, RBF mean coordinates `M`, and variance `ÏƒÂ²`, return a matrix `Î¦` of dimension `(n_nodes, n_rbf_centers+1)`. The final column is set to `1.0` to include a bias offset in addition to the RBFs.
"""
function getÎ¦Matrix(X, M, ÏƒÂ²)
    n_nodes = size(X, 1)
    n_rbf_centers = size(M,1)
    Î¦ = zeros(n_nodes, n_rbf_centers + 1)
    Î¦distances = pairwise(sqeuclidean, X, M, dims=1)

    Î¦[:, 1:end-1] .= exp.(-1.0 .* Î¦distances ./ (2ÏƒÂ²))
    Î¦[:, end] .= 1.0

    # set the last column to ones to allow for a bias term
    Î¦[:,end] .= 1
    return Î¦
end



"""
    getUMatrix(Dataset)

Perform PCA on the `Dataset` and return a matrix `U` containing the first two principal components (first two columns of data covariance matrix) and the variance of the third principal component. Size of returned matrix `U` is `(n_features, 2)`
"""
function getUMatrix(Dataset)
    pca = fit(PCA, Dataset'; maxoutdim=3)

    pca_vecs = projection(pca)  # this results the princiap component vectors (columns) sorted in order of explained variance
    pca_var = principalvars(pca)

    U = pca_vecs[:,1:2]
    for i âˆˆ axes(U,2)
        U[:,i] .= sqrt(pca_var[i]).*U[:,i]
    end
    return U, pca_var[3]
end


"""
    initWMatrix(X,Î¦,U)

Initialize parameter matrix `W`. Initial weights are chosen so that `WÎ¦'` reproduces PCA projections such that `WÎ¦' â‰ˆ UX'`
"""
function initWMatrix(X,Î¦,U)
    #return size: (n_features, n_rbf_centers+1)
    # We want to find W such that WÎ¦' = UX'
    # therefore, W' is the solution to Î¦'â‹…Î¦â‹…W' = Î¦'UX'
    return ((Î¦'*Î¦)\(Î¦'*X*U'))'
end


"""
    initÎ²â»Â¹(Î²â»Â¹, Y)

Initialized Î²â»Â¹ using our first guess for Î²â»Â¹ (from 3rd principal component variance) and the mean distance between projected rbf centers in data space.
"""
function initÎ²â»Â¹(Î²â»Â¹, Y)
    return maximum([mean(pairwise(sqeuclidean, Y, dims=2))/2, Î²â»Â¹])
end




mutable struct GenerativeTopographicMap{T<:AbstractArray, T2<:AbstractArray,  T3<:AbstractArray, T4<:AbstractArray}
    X::T
    M::T2
    ÏƒÂ²::Float64
    Î¦::T3
    W::T4
    Î²â»Â¹::Float64
    Î±::Float64
    tol::Float64
    niter::Int
    nrepeats::Int
    verbose::Bool
end



"""
    GTM(k, m, Ïƒ, Dataset; Î±=0.0, tol=0.0001, verbose=false)

Initialize hyperparameters for a GTM model.

- `k`: square root of the number of latent nodes
- `m`: square root of the number of RBF centers in latent space
- `Ïƒ`: standard deviation for latent space RBF functions
- `Dataset`: dataset to fit GTM model to. Assumed shape is `(n_datapoints, n_features)`
- `Î±`: Weight regularization parameter (`0.0` means no regularization)
- `tol`: absolute tolerance used during fitting.
- `verbose`: Set to true for extra print statements.
"""
function GenerativeTopographicMap(k, m, Ïƒ, Dataset; Î±=0.1, tol=0.0001, niter=200, nrepeats=4, verbose=false)
    n_features =  size(Dataset, 2)
    n_datapoints = size(Dataset, 1)
    n_nodes = k*k
    n_rbf_centers = m*m

    X = getCoordsMatrix(k)
    M = getCoordsMatrix(m)
    ÏƒÂ² = initializeVariance(Ïƒ, M)
    Î¦ = getÎ¦Matrix(X, M, ÏƒÂ²)
    U,Î²â»Â¹ = getUMatrix(Dataset)
    W = initWMatrix(X, Î¦, U)

    Y = W*Î¦'

    Î²â»Â¹ = initÎ²â»Â¹(Î²â»Â¹, Y)

    return GenerativeTopographicMap(X, M, ÏƒÂ², Î¦, W, Î²â»Â¹, Î±, tol, niter, nrepeats, verbose)
end



# ----------------------------------------------
#     Fitting functions
# ----------------------------------------------

"""
    getYMatrix(gtm::GenerativeTopographicMap)

Compute Gaussian centers in data space via `Y=W*Î¦'`. Return size is `(n_features, n_nodes)`.
"""
function getYMatrix(gtm::GenerativeTopographicMap)
    return gtm.W * gtm.Î¦'
end

"""
    getDMatrix(gtm::GenerativeTopographicMap, Dataset)

Compute pairwise distances between projected gaussian centers `Y` and data points in `Dataset`. Resulting size is `(n_nodes, n_datapoints)`.
"""
function getDMatrix(gtm::GenerativeTopographicMap, Dataset)
    # (n_features, n_nodes)  (n_datapoints, n_features)
    return pairwise(sqeuclidean, getYMatrix(gtm), Dataset', dims=2)
end



"""
    Posterior(gtm::GenerativeTopographicMap)

Compute a matrix of contributions to posterior probabilities. This is an intermediate result to facilitate computation of true posterior probabilities given by the responsability matrix `R`. The returned size is (n_nodes, n_datapoints). The [exp-normalize trick](https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/) is used for numerical stability.
"""
function Posterior(gtm::GenerativeTopographicMap, Dataset)
    D = getDMatrix(gtm, Dataset)
    # inner = BigFloat.(- D ./ (2*Î²â»Â¹))
    inner = - D ./ (2*gtm.Î²â»Â¹)
    maxes = maximum(inner, dims=1)

    res = inner
    for j âˆˆ axes(inner, 2)
        res[:, j] .= exp.(inner[:, j] .- maxes[j])
    end

    return res
end


function Posterior(Î²â»Â¹, D)
    # inner = BigFloat.(- D ./ (2*Î²â»Â¹))
    inner = - D ./ (2*Î²â»Â¹)
    maxes = maximum(inner, dims=1)

    res = inner
    for j âˆˆ axes(inner, 2)
        res[:, j] .= exp.(inner[:, j] .- maxes[j])
    end

    return res
end


"""
    Responsabilities(gtm::GenerativeTopographicMapping, Dataset)

Compute matrix of responsabilities of each node in `X` to datapoints in `Dataset`. Return matrix is of size `(n_nodes, n_datapoints)`.
"""
function Responsabilities(gtm::GenerativeTopographicMap, Dataset)
    # sum along rows since each column is a new data point
    # here we should use the exp-normalize trick
    P = Posterior(gtm, Dataset)
    Î£s = sum(P, dims=1)
    R = P
    for j âˆˆ axes(R,2)
        R[:,j] .= (R[:,j] ./ Î£s[j])
    end
    return R
end


function Responsabilities(P)
    # sum along rows since each column is a new data point
    # here we should use the exp-normalize trick
    Î£s = sum(P, dims=1)
    R = P
    for j âˆˆ axes(R,2)
        R[:,j] .= (R[:,j] ./ Î£s[j])
    end
    return R
end






"""
    getGMatrix(R)

Create diagonal matrix `G` from responsability matrix `R`. Return size is `(n_nodes, n_nodes)`.
"""
function getGMatrix(R)
    # return size: (n_nodes, n_nodes)
    # G is determined by the sum over data points at each node
    Î£s = vec(sum(R, dims=2))
    return diagm(Î£s)
end


"""
    updateW!(gtm::GenerativeTopographicMap, Dataset)

Update model weights using responsability matrix.
"""
function updateW!(gtm::GenerativeTopographicMap, R, Dataset)
    G = getGMatrix(R)
    LHS = gtm.Î¦'*G*gtm.Î¦
    if gtm.Î± > 0
        LHS .= LHS +  gtm.Î±*gtm.Î²â»Â¹*I
    end

    # now we have (LHS)W =  Î¦'Rğ’Ÿ
    gtm.W .= (LHS\(gtm.Î¦'*R*Dataset))'
end


"""
    updateBeta(R, D)
"""
function updateBeta!(gtm::GenerativeTopographicMap, R, D)
    ND = size(R,1)*size(R,2)
    #NOTE: this is element wise multiplication
    gtm.Î²â»Â¹ = sum(R .* D)/ND
end


"""
    estimateLogLikelihood(gtm::GenerativeTopographicMap, P, Dataset)

Compute the log-likelihood of obtaining our data provided parameters `W` and `Î²â»Â¹`.
"""
function estimateLogLikelihood(gtm::GenerativeTopographicMap, P, Dataset)
    n_nodes = size(P,1)
    n_datapoints = size(P, 2)
    n_features = size(Dataset, 2)  # number of columns for each data record

    prior = 1.0/n_nodes
    # now we need the exponential prefactor that we skipped before when calculating P matrix
    prefactor = (1/(2*Ï€*gtm.Î²â»Â¹))^(n_features/2)

    loglikelihood = sum(log.(prior * prefactor * sum(P, dims=1)))./n_datapoints  # we want to maximize log-likelihood or minimize -log-likelihood
    return loglikelihood
end


"""
    get_means(R, X)

Compute responsability weighted mean node position: ``âŸ¨x|tâ±¼, W, Î²âŸ©=Î£â±¼ Ráµ¢â±¼xáµ¢ ``
"""
function get_means(R, X)
    return R' * X
end

function get_means(gtm::GenerativeTopographicMap, Dataset)
    R = Responsabilities(gtm, Dataset)
    return R' * gtm.X
end


"""
    get_modes(R, X)

Compute the node corresponding to the mode responsability for each data point.
"""
function get_modes(R, X)
    idx = argmax(R, dims=1)
    idx = [idx[i][1] for i âˆˆ 1:length(idx)]
    return X[idx, :]
end


function get_modes(gtm::GenerativeTopographicMap, Dataset)
    R = Responsabilities(gtm, Dataset)
    X = gtm.X
    idx = argmax(R, dims=1)
    idx = [idx[i][1] for i âˆˆ 1:length(idx)]
    return X[idx, :]
end


function getModeidx(gtm::GenerativeTopographicMap, Dataset)
    R = Responsabilities(gtm, Dataset)
    idx = argmax(R, dims=1)
    return [idx[i][1] for i âˆˆ 1:length(idx)]
end



"""
    fit!(gtm::GenerativeTopographicMap, Dataset)

Fit an initialized generative topographic map `gtm` to a dataset `Dataset`.
"""
function fit!(gtm::GenerativeTopographicMap, Dataset)
    Y = gtm.W * gtm.Î¦'
    D = pairwise(sqeuclidean, Y, Dataset', dims=2)

    i = 1
    diff = 1000
    converged = 0
    minusâ„“ = 1000
    minusâ„“_prev = 1000
    while i < (gtm.niter) && converged < gtm.nrepeats

        # -------------------------------
        # Expectation step
        # -------------------------------

        Pmat = Posterior(gtm.Î²â»Â¹, D)
        R = Responsabilities(Pmat)


        # -------------------------------
        # Maximization step
        # -------------------------------
        updateW!(gtm, R, Dataset)
        Y = gtm.W*gtm.Î¦'
        D = pairwise(sqeuclidean, Y, Dataset', dims=2)
        updateBeta!(gtm, R, D)

        if i == 1
            minusâ„“ = -estimateLogLikelihood(gtm, Pmat, Dataset)
        else
            minusâ„“_prev = minusâ„“
            minusâ„“ = -estimateLogLikelihood(gtm, Pmat, Dataset)
            diff = abs(minusâ„“_prev - minusâ„“)
        end

        # we need to have 4 consecutaive updates with diff at or below the tolerance to exit
        if diff <= gtm.tol
            converged += 1
        else
            converged = 0
        end

        if gtm.verbose
            println("Iter: ", i, "  â„“: ", -minusâ„“)
        end

        i += 1
    end
end
